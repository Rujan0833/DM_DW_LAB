{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNopSY9g9dRCO8Fp6nITH+0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rujan0833/DM_DW_LAB/blob/main/LAB4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#K-Means"
      ],
      "metadata": {
        "id": "mjaMaN92UyKX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoWRe00OS_Cb",
        "outputId": "37b03c15-2c05-4a4f-907e-ee92daf47a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running K-Means with k=3 on data.csv...\n",
            "K-Means converged after 5 iterations.\n",
            "\n",
            "Final Cluster Assignments:\n",
            "[2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 2 0]\n",
            "\n",
            "Final Centroids:\n",
            "[[7.         1.        ]\n",
            " [5.88276264 5.7448722 ]\n",
            " [1.82512542 2.29187589]]\n",
            "\n",
            "DataFrame with K-Means Clusters:\n",
            "   Feature_1  Feature_2  KMeans_Cluster\n",
            "0   2.248357   1.930868               2\n",
            "1   2.323844   2.761515               2\n",
            "2   1.882923   1.882932               2\n",
            "3   2.789606   2.383717               2\n",
            "4   1.765263   2.271280               2\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def euclidean_distance(point1, point2):\n",
        "\n",
        "    return np.sqrt(np.sum((point1 - point2)**2))\n",
        "\n",
        "def assign_to_clusters(data, centroids):\n",
        "\n",
        "    assignments = np.zeros(data.shape[0], dtype=int)\n",
        "    for i, point in enumerate(data):\n",
        "        distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
        "        assignments[i] = np.argmin(distances)\n",
        "    return assignments\n",
        "\n",
        "def update_centroids(data, assignments, k):\n",
        "\n",
        "    new_centroids = np.zeros((k, data.shape[1]))\n",
        "    for cluster_id in range(k):\n",
        "        points_in_cluster = data[assignments == cluster_id]\n",
        "        if len(points_in_cluster) > 0:\n",
        "            new_centroids[cluster_id] = np.mean(points_in_cluster, axis=0)\n",
        "    return new_centroids\n",
        "\n",
        "def kmeans(data, k, max_iterations=100, random_state=None):\n",
        "\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        data_np = data.values\n",
        "    else:\n",
        "        data_np = data\n",
        "\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    # Step 1: Initialize centroids randomly\n",
        "    # Select k random data points as initial centroids\n",
        "    initial_centroid_indices = np.random.choice(data_np.shape[0], k, replace=False)\n",
        "    centroids = data_np[initial_centroid_indices]\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        # Step 2: Assign data points to the closest centroid\n",
        "        assignments = assign_to_clusters(data_np, centroids)\n",
        "\n",
        "        # Step 3: Update centroids\n",
        "        new_centroids = update_centroids(data_np, assignments, k)\n",
        "\n",
        "        # Check for convergence (if centroids don't change significantly)\n",
        "        if np.allclose(centroids, new_centroids):\n",
        "            print(f\"K-Means converged after {iteration + 1} iterations.\")\n",
        "            break\n",
        "        centroids = new_centroids\n",
        "    else:\n",
        "        print(f\"K-Means reached max iterations ({max_iterations}) without convergence.\")\n",
        "\n",
        "    return assignments, centroids\n",
        "\n",
        "# --- Example Usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the dataset\n",
        "    try:\n",
        "        df = pd.read_csv(\"data.csv\")\n",
        "        # Assuming the dataset has numerical features for clustering\n",
        "        # For demonstration, let's use all columns if they are numerical\n",
        "        # Or select specific columns if needed, e.g., df[['feature1', 'feature2']]\n",
        "        data_for_clustering = df.select_dtypes(include=np.number)\n",
        "\n",
        "        if data_for_clustering.empty:\n",
        "            print(\"No numerical columns found in data.csv for clustering.\")\n",
        "        else:\n",
        "            # Drop rows with any NaN values in the selected numerical columns\n",
        "            data_for_clustering = data_for_clustering.dropna()\n",
        "\n",
        "            if data_for_clustering.empty:\n",
        "                print(\"No valid numerical data after dropping NaNs in data.csv.\")\n",
        "            else:\n",
        "                k_value = 3  # Example: Number of clusters\n",
        "                print(f\"Running K-Means with k={k_value} on data.csv...\")\n",
        "                cluster_assignments, final_centroids = kmeans(data_for_clustering, k=k_value, random_state=42)\n",
        "\n",
        "                print(\"\\nFinal Cluster Assignments:\")\n",
        "                print(cluster_assignments)\n",
        "\n",
        "                print(\"\\nFinal Centroids:\")\n",
        "                print(final_centroids)\n",
        "\n",
        "                # You can add the cluster assignments back to your original DataFrame\n",
        "                df['KMeans_Cluster'] = pd.Series(cluster_assignments, index=data_for_clustering.index)\n",
        "                print(\"\\nDataFrame with K-Means Clusters:\")\n",
        "                print(df.head())\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: data.csv not found. Please make sure the file is in the same directory.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "# K-Means++ Clustering Algorithm\n",
        "\n"
      ],
      "metadata": {
        "id": "RZRz_cjlVCt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def euclidean_distance(point1, point2):\n",
        "\n",
        "    return np.sqrt(np.sum((point1 - point2)**2))\n",
        "\n",
        "def assign_to_clusters(data, centroids):\n",
        "\n",
        "    assignments = np.zeros(data.shape[0], dtype=int)\n",
        "    for i, point in enumerate(data):\n",
        "        distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
        "        assignments[i] = np.argmin(distances)\n",
        "    return assignments\n",
        "\n",
        "def update_centroids(data, assignments, k):\n",
        "\n",
        "    new_centroids = np.zeros((k, data.shape[1]))\n",
        "    for cluster_id in range(k):\n",
        "        points_in_cluster = data[assignments == cluster_id]\n",
        "        if len(points_in_cluster) > 0:\n",
        "            new_centroids[cluster_id] = np.mean(points_in_cluster, axis=0)\n",
        "    return new_centroids\n",
        "\n",
        "def kmeans_plusplus_init(data, k, random_state=None):\n",
        "\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    n_samples, n_features = data.shape\n",
        "    centroids = np.zeros((k, n_features))\n",
        "\n",
        "    # Step 1: Choose one center uniformly at random from the data points.\n",
        "    first_centroid_idx = np.random.choice(n_samples)\n",
        "    centroids[0] = data[first_centroid_idx]\n",
        "\n",
        "    # Step 2: For each data point x, compute D(x), the distance between x and the nearest center that has already been chosen.\n",
        "    # Step 3: Choose new center x' with probability proportional to D(x)^2.\n",
        "    # Step 4: Repeat Steps 2 and 3 until k centers have been chosen.\n",
        "    for i in range(1, k):\n",
        "        distances_sq = np.array([min([euclidean_distance(point, c)**2 for c in centroids[:i]]) for point in data])\n",
        "        probabilities = distances_sq / np.sum(distances_sq)\n",
        "        next_centroid_idx = np.random.choice(n_samples, p=probabilities)\n",
        "        centroids[i] = data[next_centroid_idx]\n",
        "    return centroids\n",
        "\n",
        "def kmeans_plusplus(data, k, max_iterations=100, random_state=None):\n",
        "\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        data_np = data.values\n",
        "    else:\n",
        "        data_np = data\n",
        "\n",
        "    # Step 1: Initialize centroids using K-Means++ method\n",
        "    centroids = kmeans_plusplus_init(data_np, k, random_state=random_state)\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        # Step 2: Assign data points to the closest centroid\n",
        "        assignments = assign_to_clusters(data_np, centroids)\n",
        "\n",
        "        # Step 3: Update centroids\n",
        "        new_centroids = update_centroids(data_np, assignments, k)\n",
        "\n",
        "        # Check for convergence (if centroids don't change significantly)\n",
        "        if np.allclose(centroids, new_centroids):\n",
        "            print(f\"K-Means++ converged after {iteration + 1} iterations.\")\n",
        "            break\n",
        "        centroids = new_centroids\n",
        "    else:\n",
        "        print(f\"K-Means++ reached max iterations ({max_iterations}) without convergence.\")\n",
        "\n",
        "    return assignments, centroids\n",
        "\n",
        "# --- Example Usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the dataset\n",
        "    try:\n",
        "        df = pd.read_csv(\"data.csv\")\n",
        "        # Assuming the dataset has numerical features for clustering\n",
        "        data_for_clustering = df.select_dtypes(include=np.number)\n",
        "\n",
        "        if data_for_clustering.empty:\n",
        "            print(\"No numerical columns found in data.csv for clustering.\")\n",
        "        else:\n",
        "            # Drop rows with any NaN values in the selected numerical columns\n",
        "            data_for_clustering = data_for_clustering.dropna()\n",
        "\n",
        "            if data_for_clustering.empty:\n",
        "                print(\"No valid numerical data after dropping NaNs in data.csv.\")\n",
        "            else:\n",
        "                k_value = 3  # Example: Number of clusters\n",
        "                print(f\"Running K-Means++ with k={k_value} on data.csv...\")\n",
        "                cluster_assignments, final_centroids = kmeans_plusplus(data_for_clustering, k=k_value, random_state=42)\n",
        "\n",
        "                print(\"\\nFinal Cluster Assignments (K-Means++):\")\n",
        "                print(cluster_assignments)\n",
        "\n",
        "                print(\"\\nFinal Centroids (K-Means++):\")\n",
        "                print(final_centroids)\n",
        "\n",
        "                # You can add the cluster assignments back to your original DataFrame\n",
        "                df['KMeans++_Cluster'] = pd.Series(cluster_assignments, index=data_for_clustering.index)\n",
        "                print(\"\\nDataFrame with K-Means++ Clusters:\")\n",
        "                print(df.head())\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: data.csv not found. Please make sure the file is in the same directory.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVQ_rsIbUuBj",
        "outputId": "ddbd9ee4-4c4e-4855-b9dd-ace50e0bf25a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running K-Means++ with k=3 on data.csv...\n",
            "K-Means++ converged after 4 iterations.\n",
            "\n",
            "Final Cluster Assignments (K-Means++):\n",
            "[0 0 0 0 0 0 0 0 0 0 2 1 2 2 2 2 2 1 1 2 0 1]\n",
            "\n",
            "Final Centroids (K-Means++):\n",
            "[[1.82512542 2.29187589]\n",
            " [6.44231392 3.94407916]\n",
            " [5.72262439 6.0960579 ]]\n",
            "\n",
            "DataFrame with K-Means++ Clusters:\n",
            "   Feature_1  Feature_2  KMeans++_Cluster\n",
            "0   2.248357   1.930868                 0\n",
            "1   2.323844   2.761515                 0\n",
            "2   1.882923   1.882932                 0\n",
            "3   2.789606   2.383717                 0\n",
            "4   1.765263   2.271280                 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#K-Medoids (PAM) Clustering Algorithm"
      ],
      "metadata": {
        "id": "yaRlgOSdWAYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def manhattan_distance(point1, point2):\n",
        "    return np.sum(np.abs(point1 - point2))\n",
        "\n",
        "def calculate_cost(data, medoids, assignments):\n",
        "    cost = 0.0\n",
        "    for i, point in enumerate(data):\n",
        "        medoid_idx = assignments[i]\n",
        "        cost += manhattan_distance(point, medoids[medoid_idx])\n",
        "    return cost\n",
        "\n",
        "def kmedoids(data, k, max_iterations=100, random_state=None):\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        data_np = data.values\n",
        "    else:\n",
        "        data_np = data\n",
        "\n",
        "    n_samples, n_features = data_np.shape\n",
        "\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    current_medoid_indices = np.random.choice(n_samples, k, replace=False)\n",
        "    current_medoids = data_np[current_medoid_indices]\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        assignments = np.zeros(n_samples, dtype=int)\n",
        "        for i, point in enumerate(data_np):\n",
        "            distances = [manhattan_distance(point, medoid) for medoid in current_medoids]\n",
        "            assignments[i] = np.argmin(distances)\n",
        "\n",
        "        current_cost = calculate_cost(data_np, current_medoids, assignments)\n",
        "\n",
        "        improved = False\n",
        "        for m_idx in range(k):\n",
        "            original_medoid = current_medoids[m_idx]\n",
        "            potential_new_medoid_indices = [i for i in range(n_samples) if i not in current_medoid_indices]\n",
        "\n",
        "            for p_idx in potential_new_medoid_indices:\n",
        "                potential_new_medoid = data_np[p_idx]\n",
        "\n",
        "                temp_medoids = np.copy(current_medoids)\n",
        "                temp_medoids[m_idx] = potential_new_medoid\n",
        "\n",
        "                temp_assignments = np.zeros(n_samples, dtype=int)\n",
        "                for i, point in enumerate(data_np):\n",
        "                    distances = [manhattan_distance(point, medoid) for medoid in temp_medoids]\n",
        "                    temp_assignments[i] = np.argmin(distances)\n",
        "\n",
        "                temp_cost = calculate_cost(data_np, temp_medoids, temp_assignments)\n",
        "\n",
        "                if temp_cost < current_cost:\n",
        "                    current_cost = temp_cost\n",
        "                    current_medoids = temp_medoids\n",
        "                    current_medoid_indices[m_idx] = p_idx\n",
        "                    assignments = temp_assignments\n",
        "                    improved = True\n",
        "                    break\n",
        "            if improved:\n",
        "                break\n",
        "\n",
        "        if not improved:\n",
        "            print(f\"K-Medoids converged after {iteration + 1} iterations.\")\n",
        "            break\n",
        "    else:\n",
        "        print(f\"K-Medoids reached max iterations ({max_iterations}) without convergence.\")\n",
        "\n",
        "    return assignments, current_medoids\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        df = pd.read_csv(\"data.csv\")\n",
        "        data_for_clustering = df.select_dtypes(include=np.number)\n",
        "\n",
        "        if data_for_clustering.empty:\n",
        "            print(\"No numerical columns found in data.csv for clustering.\")\n",
        "        else:\n",
        "            data_for_clustering = data_for_clustering.dropna()\n",
        "\n",
        "            if data_for_clustering.empty:\n",
        "                print(\"No valid numerical data after dropping NaNs in data.csv.\")\n",
        "            else:\n",
        "                k_value = 3\n",
        "                print(f\"Running K-Medoids with k={k_value} on data.csv...\")\n",
        "                cluster_assignments, final_medoids = kmedoids(data_for_clustering, k=k_value, random_state=42)\n",
        "\n",
        "                print(\"\\nFinal Cluster Assignments (K-Medoids):\")\n",
        "                print(cluster_assignments)\n",
        "\n",
        "                print(\"\\nFinal Medoids (K-Medoids):\")\n",
        "                print(final_medoids)\n",
        "\n",
        "                df['KMedoids_Cluster'] = pd.Series(cluster_assignments, index=data_for_clustering.index)\n",
        "                print(\"\\nDataFrame with K-Medoids Clusters:\")\n",
        "                print(df.head())\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: data.csv not found. Please make sure the file is in the same directory.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPyO_fETWcXO",
        "outputId": "f369f5c2-5c50-4ff4-f1b8-d92fed7c7ff1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running K-Medoids with k=3 on data.csv...\n",
            "K-Medoids converged after 7 iterations.\n",
            "\n",
            "Final Cluster Assignments (K-Medoids):\n",
            "[2 2 2 2 2 2 2 2 2 2 0 0 1 1 1 1 0 0 0 1 1 0]\n",
            "\n",
            "Final Medoids (K-Medoids):\n",
            "[[6.57578144 5.14540945]\n",
            " [5.61893209 6.07764581]\n",
            " [1.88292331 1.88293152]]\n",
            "\n",
            "DataFrame with K-Medoids Clusters:\n",
            "   Feature_1  Feature_2  KMedoids_Cluster\n",
            "0   2.248357   1.930868                 2\n",
            "1   2.323844   2.761515                 2\n",
            "2   1.882923   1.882932                 2\n",
            "3   2.789606   2.383717                 2\n",
            "4   1.765263   2.271280                 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Agglomerative Hierarchical Clustering Algorithm"
      ],
      "metadata": {
        "id": "_aL8XKcPWq3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "def euclidean_distance(point1, point2):\n",
        "    return np.sqrt(np.sum((point1 - point2)**2))\n",
        "\n",
        "def single_linkage(cluster1_points, cluster2_points):\n",
        "    min_dist = float('inf')\n",
        "    for p1 in cluster1_points:\n",
        "        for p2 in cluster2_points:\n",
        "            dist = euclidean_distance(p1, p2)\n",
        "            if dist < min_dist:\n",
        "                min_dist = dist\n",
        "    return min_dist\n",
        "\n",
        "def complete_linkage(cluster1_points, cluster2_points):\n",
        "    max_dist = 0.0\n",
        "    for p1 in cluster1_points:\n",
        "        for p2 in cluster2_points:\n",
        "            dist = euclidean_distance(p1, p2)\n",
        "            if dist > max_dist:\n",
        "                max_dist = dist\n",
        "    return max_dist\n",
        "\n",
        "def average_linkage(cluster1_points, cluster2_points):\n",
        "    total_dist = 0.0\n",
        "    count = 0\n",
        "    for p1 in cluster1_points:\n",
        "        for p2 in cluster2_points:\n",
        "            total_dist += euclidean_distance(p1, p2)\n",
        "            count += 1\n",
        "    return total_dist / count if count > 0 else 0.0\n",
        "\n",
        "def agglomerative_clustering(data, n_clusters, linkage_method='single'):\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        data_np = data.values\n",
        "    else:\n",
        "        data_np = data\n",
        "\n",
        "    n_samples = data_np.shape[0]\n",
        "\n",
        "    clusters = [[i] for i in range(n_samples)]\n",
        "\n",
        "    if linkage_method == 'single':\n",
        "        linkage_func = single_linkage\n",
        "    elif linkage_method == 'complete':\n",
        "        linkage_func = complete_linkage\n",
        "    elif linkage_method == 'average':\n",
        "        linkage_func = average_linkage\n",
        "    else:\n",
        "        raise ValueError(\"Invalid linkage_method. Choose from 'single', 'complete', 'average'.\")\n",
        "\n",
        "    while len(clusters) > n_clusters:\n",
        "        min_dist = float('inf')\n",
        "        merge_c1_idx, merge_c2_idx = -1, -1\n",
        "\n",
        "        for i in range(len(clusters)):\n",
        "            for j in range(i + 1, len(clusters)):\n",
        "                c1_points = data_np[clusters[i]]\n",
        "                c2_points = data_np[clusters[j]]\n",
        "                dist = linkage_func(c1_points, c2_points)\n",
        "\n",
        "                if dist < min_dist:\n",
        "                    min_dist = dist\n",
        "                    merge_c1_idx = i\n",
        "                    merge_c2_idx = j\n",
        "\n",
        "        if merge_c1_idx != -1 and merge_c2_idx != -1:\n",
        "            new_cluster = clusters[merge_c1_idx] + clusters[merge_c2_idx]\n",
        "            if merge_c1_idx < merge_c2_idx:\n",
        "                del clusters[merge_c2_idx]\n",
        "                del clusters[merge_c1_idx]\n",
        "            else:\n",
        "                del clusters[merge_c1_idx]\n",
        "                del clusters[merge_c2_idx]\n",
        "            clusters.append(new_cluster)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    assignments = np.zeros(n_samples, dtype=int)\n",
        "    for cluster_id, point_indices in enumerate(clusters):\n",
        "        for idx in point_indices:\n",
        "            assignments[idx] = cluster_id\n",
        "\n",
        "    return assignments\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        df = pd.read_csv(\"data.csv\")\n",
        "        data_for_clustering = df.select_dtypes(include=np.number)\n",
        "\n",
        "        if data_for_clustering.empty:\n",
        "            print(\"No numerical columns found in data.csv for clustering.\")\n",
        "        else:\n",
        "            data_for_clustering = data_for_clustering.dropna()\n",
        "\n",
        "            if data_for_clustering.empty:\n",
        "                print(\"No valid numerical data after dropping NaNs in data.csv.\")\n",
        "            else:\n",
        "                n_clusters_value = 3\n",
        "                print(f\"Running Agglomerative Hierarchical Clustering (Single Linkage) with {n_clusters_value} clusters on data.csv...\")\n",
        "                cluster_assignments_single = agglomerative_clustering(data_for_clustering, n_clusters=n_clusters_value, linkage_method='single')\n",
        "\n",
        "                print(\"\\nFinal Cluster Assignments (Agglomerative - Single Linkage):\")\n",
        "                print(cluster_assignments_single)\n",
        "\n",
        "                df['Agglomerative_Single_Cluster'] = pd.Series(cluster_assignments_single, index=data_for_clustering.index)\n",
        "                print(\"\\nDataFrame with Agglomerative (Single Linkage) Clusters:\")\n",
        "                print(df.head())\n",
        "\n",
        "                print(f\"\\nRunning Agglomerative Hierarchical Clustering (Complete Linkage) with {n_clusters_value} clusters on data.csv...\")\n",
        "                cluster_assignments_complete = agglomerative_clustering(data_for_clustering, n_clusters=n_clusters_value, linkage_method='complete')\n",
        "\n",
        "                print(\"\\nFinal Cluster Assignments (Agglomerative - Complete Linkage):\")\n",
        "                print(cluster_assignments_complete)\n",
        "\n",
        "                df['Agglomerative_Complete_Cluster'] = pd.Series(cluster_assignments_complete, index=data_for_clustering.index)\n",
        "                print(\"\\nDataFrame with Agglomerative (Complete Linkage) Clusters:\")\n",
        "                print(df.head())\n",
        "\n",
        "                print(f\"\\nRunning Agglomerative Hierarchical Clustering (Average Linkage) with {n_clusters_value} clusters on data.csv...\")\n",
        "                cluster_assignments_average = agglomerative_clustering(data_for_clustering, n_clusters=n_clusters_value, linkage_method='average')\n",
        "\n",
        "                print(\"\\nFinal Cluster Assignments (Agglomerative - Average Linkage):\")\n",
        "                print(cluster_assignments_average)\n",
        "\n",
        "                df['Agglomerative_Average_Cluster'] = pd.Series(cluster_assignments_average, index=data_for_clustering.index)\n",
        "                print(\"\\nDataFrame with Agglomerative (Average Linkage) Clusters:\")\n",
        "                print(df.head())\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: data.csv not found. Please make sure the file is in the same directory.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GJqgsSXWu1_",
        "outputId": "98ac7893-da43-4377-aaa4-612928b0f7a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Agglomerative Hierarchical Clustering (Single Linkage) with 3 clusters on data.csv...\n",
            "\n",
            "Final Cluster Assignments (Agglomerative - Single Linkage):\n",
            "[2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 2 0]\n",
            "\n",
            "DataFrame with Agglomerative (Single Linkage) Clusters:\n",
            "   Feature_1  Feature_2  Agglomerative_Single_Cluster\n",
            "0   2.248357   1.930868                             2\n",
            "1   2.323844   2.761515                             2\n",
            "2   1.882923   1.882932                             2\n",
            "3   2.789606   2.383717                             2\n",
            "4   1.765263   2.271280                             2\n",
            "\n",
            "Running Agglomerative Hierarchical Clustering (Complete Linkage) with 3 clusters on data.csv...\n",
            "\n",
            "Final Cluster Assignments (Agglomerative - Complete Linkage):\n",
            "[2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 2 0]\n",
            "\n",
            "DataFrame with Agglomerative (Complete Linkage) Clusters:\n",
            "   Feature_1  Feature_2  Agglomerative_Single_Cluster  \\\n",
            "0   2.248357   1.930868                             2   \n",
            "1   2.323844   2.761515                             2   \n",
            "2   1.882923   1.882932                             2   \n",
            "3   2.789606   2.383717                             2   \n",
            "4   1.765263   2.271280                             2   \n",
            "\n",
            "   Agglomerative_Complete_Cluster  \n",
            "0                               2  \n",
            "1                               2  \n",
            "2                               2  \n",
            "3                               2  \n",
            "4                               2  \n",
            "\n",
            "Running Agglomerative Hierarchical Clustering (Average Linkage) with 3 clusters on data.csv...\n",
            "\n",
            "Final Cluster Assignments (Agglomerative - Average Linkage):\n",
            "[2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 2 0]\n",
            "\n",
            "DataFrame with Agglomerative (Average Linkage) Clusters:\n",
            "   Feature_1  Feature_2  Agglomerative_Single_Cluster  \\\n",
            "0   2.248357   1.930868                             2   \n",
            "1   2.323844   2.761515                             2   \n",
            "2   1.882923   1.882932                             2   \n",
            "3   2.789606   2.383717                             2   \n",
            "4   1.765263   2.271280                             2   \n",
            "\n",
            "   Agglomerative_Complete_Cluster  Agglomerative_Average_Cluster  \n",
            "0                               2                              2  \n",
            "1                               2                              2  \n",
            "2                               2                              2  \n",
            "3                               2                              2  \n",
            "4                               2                              2  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Divisive Hierarchical Clustering Algorithm\n"
      ],
      "metadata": {
        "id": "173F8rNQWyyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def euclidean_distance(point1, point2):\n",
        "    return np.sqrt(np.sum((point1 - point2)**2))\n",
        "\n",
        "def assign_to_clusters(data, centroids):\n",
        "    assignments = np.zeros(data.shape[0], dtype=int)\n",
        "    for i, point in enumerate(data):\n",
        "        distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
        "        assignments[i] = np.argmin(distances)\n",
        "    return assignments\n",
        "\n",
        "def update_centroids(data, assignments, k):\n",
        "    new_centroids = np.zeros((k, data.shape[1]))\n",
        "    for cluster_id in range(k):\n",
        "        points_in_cluster = data[assignments == cluster_id]\n",
        "        if len(points_in_cluster) > 0:\n",
        "            new_centroids[cluster_id] = np.mean(points_in_cluster, axis=0)\n",
        "    return new_centroids\n",
        "\n",
        "def kmeans_simple(data, k, max_iterations=50, random_state=None):\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    n_samples = data.shape[0]\n",
        "    if n_samples < k:\n",
        "        return np.zeros(n_samples, dtype=int), data\n",
        "\n",
        "    initial_centroid_indices = np.random.choice(n_samples, k, replace=False)\n",
        "    centroids = data[initial_centroid_indices]\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "        assignments = assign_to_clusters(data, centroids)\n",
        "        new_centroids = update_centroids(data, assignments, k)\n",
        "        if np.allclose(centroids, new_centroids):\n",
        "            break\n",
        "        centroids = new_centroids\n",
        "    return assignments, centroids\n",
        "\n",
        "\n",
        "def divisive_clustering(data, n_clusters, random_state=None):\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        data_np = data.values\n",
        "    else:\n",
        "        data_np = data\n",
        "\n",
        "    n_samples = data_np.shape[0]\n",
        "    if n_samples == 0:\n",
        "        return np.array([])\n",
        "\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    current_clusters = {0: list(range(n_samples))}\n",
        "    cluster_id_counter = 1\n",
        "\n",
        "    while len(current_clusters) < n_clusters:\n",
        "        if not current_clusters:\n",
        "            break\n",
        "\n",
        "        largest_cluster_id = -1\n",
        "        max_points = -1\n",
        "        for c_id, indices in current_clusters.items():\n",
        "            if len(indices) > max_points:\n",
        "                max_points = len(indices)\n",
        "                largest_cluster_id = c_id\n",
        "\n",
        "        if largest_cluster_id == -1 or max_points < 2:\n",
        "            break\n",
        "\n",
        "        cluster_to_split_indices = current_clusters[largest_cluster_id]\n",
        "        cluster_to_split_data = data_np[cluster_to_split_indices]\n",
        "\n",
        "        if len(cluster_to_split_data) < 2:\n",
        "            break\n",
        "\n",
        "        sub_assignments, _ = kmeans_simple(cluster_to_split_data, k=2, random_state=random_state)\n",
        "\n",
        "        new_cluster1_indices = [cluster_to_split_indices[i] for i, assign in enumerate(sub_assignments) if assign == 0]\n",
        "        new_cluster2_indices = [cluster_to_split_indices[i] for i, assign in enumerate(sub_assignments) if assign == 1]\n",
        "\n",
        "        del current_clusters[largest_cluster_id]\n",
        "        if new_cluster1_indices:\n",
        "            current_clusters[cluster_id_counter] = new_cluster1_indices\n",
        "            cluster_id_counter += 1\n",
        "        if new_cluster2_indices:\n",
        "            current_clusters[cluster_id_counter] = new_cluster2_indices\n",
        "            cluster_id_counter += 1\n",
        "\n",
        "        if len(current_clusters) == n_clusters:\n",
        "            break\n",
        "\n",
        "    final_assignments = np.zeros(n_samples, dtype=int)\n",
        "    for cluster_id, original_indices in current_clusters.items():\n",
        "        for original_idx in original_indices:\n",
        "            final_assignments[original_idx] = cluster_id\n",
        "\n",
        "    return final_assignments\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        df = pd.read_csv(\"data.csv\")\n",
        "        data_for_clustering = df.select_dtypes(include=np.number)\n",
        "\n",
        "        if data_for_clustering.empty:\n",
        "            print(\"No numerical columns found in data.csv for clustering.\")\n",
        "        else:\n",
        "            data_for_clustering = data_for_clustering.dropna()\n",
        "\n",
        "            if data_for_clustering.empty:\n",
        "                print(\"No valid numerical data after dropping NaNs in data.csv.\")\n",
        "            else:\n",
        "                n_clusters_value = 3\n",
        "                print(f\"Running Divisive Hierarchical Clustering with {n_clusters_value} clusters on data.csv...\")\n",
        "                cluster_assignments = divisive_clustering(data_for_clustering, n_clusters=n_clusters_value, random_state=42)\n",
        "\n",
        "                print(\"\\nFinal Cluster Assignments (Divisive Clustering):\")\n",
        "                print(cluster_assignments)\n",
        "\n",
        "                df['Divisive_Cluster'] = pd.Series(cluster_assignments, index=data_for_clustering.index)\n",
        "                print(\"\\nDataFrame with Divisive Clusters:\")\n",
        "                print(df.head())\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: data.csv not found. Please make sure the file is in the same directory.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Wnw6s8cW4bk",
        "outputId": "91d57bb4-577d-4372-ecda-4f5e46b4a4e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Divisive Hierarchical Clustering with 3 clusters on data.csv...\n",
            "\n",
            "Final Cluster Assignments (Divisive Clustering):\n",
            "[3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 4 2]\n",
            "\n",
            "DataFrame with Divisive Clusters:\n",
            "   Feature_1  Feature_2  Divisive_Cluster\n",
            "0   2.248357   1.930868                 3\n",
            "1   2.323844   2.761515                 3\n",
            "2   1.882923   1.882932                 3\n",
            "3   2.789606   2.383717                 3\n",
            "4   1.765263   2.271280                 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DBSCAN Clustering Algorithm"
      ],
      "metadata": {
        "id": "pTVmgHhAW75g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def euclidean_distance(point1, point2):\n",
        "    return np.sqrt(np.sum((point1 - point2)**2))\n",
        "\n",
        "def get_neighbors(data, point_idx, eps):\n",
        "    neighbors = []\n",
        "    for i in range(data.shape[0]):\n",
        "        if i == point_idx:\n",
        "            continue\n",
        "        if euclidean_distance(data[point_idx], data[i]) <= eps:\n",
        "            neighbors.append(i)\n",
        "    return neighbors\n",
        "\n",
        "def dbscan(data, eps, min_samples):\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        data_np = data.values\n",
        "    else:\n",
        "        data_np = data\n",
        "\n",
        "    n_samples = data_np.shape[0]\n",
        "    labels = -2 * np.ones(n_samples, dtype=int)\n",
        "    cluster_id = 0\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        if labels[i] != -2:\n",
        "            continue\n",
        "\n",
        "        neighbors = get_neighbors(data_np, i, eps)\n",
        "\n",
        "        if len(neighbors) < min_samples:\n",
        "            labels[i] = -1\n",
        "        else:\n",
        "            labels[i] = cluster_id\n",
        "            queue = list(neighbors)\n",
        "\n",
        "            while queue:\n",
        "                current_neighbor_idx = queue.pop(0)\n",
        "\n",
        "                if labels[current_neighbor_idx] == -1:\n",
        "                    labels[current_neighbor_idx] = cluster_id\n",
        "\n",
        "                if labels[current_neighbor_idx] != -2:\n",
        "                    continue\n",
        "\n",
        "                labels[current_neighbor_idx] = cluster_id\n",
        "\n",
        "                next_neighbors = get_neighbors(data_np, current_neighbor_idx, eps)\n",
        "\n",
        "                if len(next_neighbors) >= min_samples:\n",
        "                    for nn_idx in next_neighbors:\n",
        "                        if labels[nn_idx] == -2 or labels[nn_idx] == -1:\n",
        "                            queue.append(nn_idx)\n",
        "            cluster_id += 1\n",
        "\n",
        "    return labels\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        df2 = pd.read_csv(\"data2.csv\")\n",
        "        data_for_dbscan = df2.select_dtypes(include=np.number)\n",
        "\n",
        "        if data_for_dbscan.empty:\n",
        "            print(\"No numerical columns found in data2.csv for clustering.\")\n",
        "        else:\n",
        "            data_for_dbscan = data_for_dbscan.dropna()\n",
        "\n",
        "            if data_for_dbscan.empty:\n",
        "                print(\"No valid numerical data after dropping NaNs in data2.csv.\")\n",
        "            else:\n",
        "                eps_value = 0.5\n",
        "                min_samples_value = 5\n",
        "\n",
        "                print(f\"Running DBSCAN with eps={eps_value}, min_samples={min_samples_value} on data2.csv...\")\n",
        "                cluster_assignments = dbscan(data_for_dbscan, eps=eps_value, min_samples=min_samples_value)\n",
        "\n",
        "                print(\"\\nFinal Cluster Assignments (DBSCAN):\")\n",
        "                print(cluster_assignments)\n",
        "                print(\"Note: -1 indicates noise points.\")\n",
        "\n",
        "                df2['DBSCAN_Cluster'] = pd.Series(cluster_assignments, index=data_for_dbscan.index)\n",
        "                print(\"\\nDataFrame with DBSCAN Clusters:\")\n",
        "                print(df2.head())\n",
        "\n",
        "                unique_labels, counts = np.unique(cluster_assignments, return_counts=True)\n",
        "                print(\"\\nCluster distribution:\")\n",
        "                for label, count in zip(unique_labels, counts):\n",
        "                    if label == -1:\n",
        "                        print(f\"Noise points: {count}\")\n",
        "                    else:\n",
        "                        print(f\"Cluster {label}: {count}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: data2.csv not found. Please make sure the file is in the same directory.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqh2omciW_zc",
        "outputId": "864ea255-91ec-4586-b487-46b6842c062c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running DBSCAN with eps=0.5, min_samples=5 on data2.csv...\n",
            "\n",
            "Final Cluster Assignments (DBSCAN):\n",
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0 -1 -1 -1]\n",
            "Note: -1 indicates noise points.\n",
            "\n",
            "DataFrame with DBSCAN Clusters:\n",
            "   Feature_1  Feature_2  DBSCAN_Cluster\n",
            "0  -1.049426   0.084443               0\n",
            "1   0.922818   0.457489               0\n",
            "2   0.656787   0.699597               0\n",
            "3   1.188940  -0.386528               0\n",
            "4   0.289265  -0.137745               0\n",
            "\n",
            "Cluster distribution:\n",
            "Noise points: 3\n",
            "Cluster 0: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Outlier Analysis using Z-score Method\n"
      ],
      "metadata": {
        "id": "TH8bvrsbXB-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def zscore_outlier_detection(data, threshold=3.0):\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        outliers_df = pd.DataFrame(index=data.index, columns=data.columns, dtype=bool)\n",
        "        zscores_df = pd.DataFrame(index=data.index, columns=data.columns, dtype=float)\n",
        "\n",
        "        numerical_cols = data.select_dtypes(include=np.number).columns\n",
        "        if numerical_cols.empty:\n",
        "            print(\"Warning: No numerical columns found for Z-score outlier detection.\")\n",
        "            return outliers_df, zscores_df\n",
        "\n",
        "        for col in numerical_cols:\n",
        "            series = data[col].dropna()\n",
        "            if series.empty:\n",
        "                outliers_df[col] = False\n",
        "                zscores_df[col] = np.nan\n",
        "                continue\n",
        "\n",
        "            mean_val = series.mean()\n",
        "            std_dev = series.std()\n",
        "\n",
        "            if std_dev == 0:\n",
        "                zscores = pd.Series(0.0, index=series.index)\n",
        "                is_outlier = pd.Series(False, index=series.index)\n",
        "            else:\n",
        "                zscores = (series - mean_val) / std_dev\n",
        "                is_outlier = np.abs(zscores) > threshold\n",
        "\n",
        "            outliers_df[col] = is_outlier.reindex(data.index, fill_value=False)\n",
        "            zscores_df[col] = zscores.reindex(data.index, fill_value=np.nan)\n",
        "        return outliers_df, zscores_df\n",
        "\n",
        "    elif isinstance(data, pd.Series):\n",
        "        series = data.dropna()\n",
        "        if series.empty:\n",
        "            return pd.Series(False, index=data.index), pd.Series(np.nan, index=data.index)\n",
        "\n",
        "        mean_val = series.mean()\n",
        "        std_dev = series.std()\n",
        "\n",
        "        if std_dev == 0:\n",
        "            zscores = pd.Series(0.0, index=series.index)\n",
        "            is_outlier = pd.Series(False, index=series.index)\n",
        "        else:\n",
        "            zscores = (series - mean_val) / std_dev\n",
        "            is_outlier = np.abs(zscores) > threshold\n",
        "\n",
        "        return is_outlier.reindex(data.index, fill_value=False), zscores.reindex(data.index, fill_value=np.nan)\n",
        "\n",
        "    elif isinstance(data, np.ndarray):\n",
        "        if data.ndim > 1:\n",
        "            print(\"Warning: Z-score detection for NumPy array assumes 1D data. Processing each column if 2D.\")\n",
        "            outliers_array = np.zeros_like(data, dtype=bool)\n",
        "            zscores_array = np.zeros_like(data, dtype=float)\n",
        "            for col_idx in range(data.shape[1]):\n",
        "                col_data = data[:, col_idx]\n",
        "                mean_val = np.nanmean(col_data)\n",
        "                std_dev = np.nanstd(col_data)\n",
        "\n",
        "                if std_dev == 0:\n",
        "                    zscores_array[:, col_idx] = 0.0\n",
        "                    outliers_array[:, col_idx] = False\n",
        "                else:\n",
        "                    zscores = (col_data - mean_val) / std_dev\n",
        "                    outliers_array[:, col_idx] = np.abs(zscores) > threshold\n",
        "                    zscores_array[:, col_idx] = zscores\n",
        "            return outliers_array, zscores_array\n",
        "        else:\n",
        "            mean_val = np.nanmean(data)\n",
        "            std_dev = np.nanstd(data)\n",
        "            if std_dev == 0:\n",
        "                return np.full_like(data, False, dtype=bool), np.full_like(data, 0.0, dtype=float)\n",
        "            zscores = (data - mean_val) / std_dev\n",
        "            is_outlier = np.abs(zscores) > threshold\n",
        "            return is_outlier, zscores\n",
        "    else:\n",
        "        raise TypeError(\"Input data must be a pandas Series, DataFrame, or NumPy array.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        df3 = pd.read_csv(\"data3.csv\")\n",
        "        print(\"Original data from data3.csv:\")\n",
        "        print(df3.head())\n",
        "\n",
        "        numerical_data = df3.select_dtypes(include=np.number)\n",
        "\n",
        "        if numerical_data.empty:\n",
        "            print(\"No numerical columns found in data3.csv for Z-score outlier detection.\")\n",
        "        else:\n",
        "            zscore_threshold = 3.0\n",
        "            print(f\"\\nPerforming Z-score outlier detection with threshold={zscore_threshold}...\")\n",
        "            is_outlier_zscore, zscores = zscore_outlier_detection(numerical_data, threshold=zscore_threshold)\n",
        "\n",
        "            print(\"\\nIs Outlier (Z-score method):\")\n",
        "            print(is_outlier_zscore.head())\n",
        "\n",
        "            print(\"\\nCalculated Z-scores:\")\n",
        "            print(zscores.head())\n",
        "\n",
        "            outlier_rows_zscore = df3[is_outlier_zscore.any(axis=1)]\n",
        "            if not outlier_rows_zscore.empty:\n",
        "                print(\"\\nRows identified as outliers (Z-score method):\")\n",
        "                print(outlier_rows_zscore)\n",
        "            else:\n",
        "                print(\"\\nNo outliers found using Z-score method with the given threshold.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: data3.csv not found. Please make sure the file is in the same directory.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NA7-OYQwXEvd",
        "outputId": "9e7847ce-98ea-406e-8c36-bdbd84d0b828"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data from data3.csv:\n",
            "   Value\n",
            "0    1.0\n",
            "1    2.0\n",
            "2    3.0\n",
            "3    4.0\n",
            "4   10.0\n",
            "\n",
            "Performing Z-score outlier detection with threshold=3.0...\n",
            "\n",
            "Is Outlier (Z-score method):\n",
            "   Value\n",
            "0  False\n",
            "1  False\n",
            "2  False\n",
            "3  False\n",
            "4  False\n",
            "\n",
            "Calculated Z-scores:\n",
            "      Value\n",
            "0  0.045781\n",
            "1  0.180167\n",
            "2  0.314553\n",
            "3  0.448939\n",
            "4  1.255256\n",
            "\n",
            "No outliers found using Z-score method with the given threshold.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Outlier Detection using IQR Method"
      ],
      "metadata": {
        "id": "F_fhWE_RXHXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def iqr_outlier_detection(data, k=1.5):\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        outliers_df = pd.DataFrame(index=data.index, columns=data.columns, dtype=bool)\n",
        "        bounds_info = {}\n",
        "\n",
        "        numerical_cols = data.select_dtypes(include=np.number).columns\n",
        "        if numerical_cols.empty:\n",
        "            print(\"Warning: No numerical columns found for IQR outlier detection.\")\n",
        "            return outliers_df, bounds_info\n",
        "\n",
        "        for col in numerical_cols:\n",
        "            series = data[col].dropna()\n",
        "            if series.empty:\n",
        "                outliers_df[col] = False\n",
        "                bounds_info[col] = {'Q1': np.nan, 'Q3': np.nan, 'IQR': np.nan, 'Lower_Bound': np.nan, 'Upper_Bound': np.nan}\n",
        "                continue\n",
        "\n",
        "            Q1 = np.percentile(series, 25)\n",
        "            Q3 = np.percentile(series, 75)\n",
        "            IQR = Q3 - Q1\n",
        "\n",
        "            lower_bound = Q1 - k * IQR\n",
        "            upper_bound = Q3 + k * IQR\n",
        "\n",
        "            is_outlier = (series < lower_bound) | (series > upper_bound)\n",
        "            outliers_df[col] = is_outlier.reindex(data.index, fill_value=False)\n",
        "\n",
        "            bounds_info[col] = {\n",
        "                'Q1': Q1,\n",
        "                'Q3': Q3,\n",
        "                'IQR': IQR,\n",
        "                'Lower_Bound': lower_bound,\n",
        "                'Upper_Bound': upper_bound\n",
        "            }\n",
        "        return outliers_df, bounds_info\n",
        "\n",
        "    elif isinstance(data, pd.Series):\n",
        "        series = data.dropna()\n",
        "        if series.empty:\n",
        "            return pd.Series(False, index=data.index), {'Q1': np.nan, 'Q3': np.nan, 'IQR': np.nan, 'Lower_Bound': np.nan, 'Upper_Bound': np.nan}\n",
        "\n",
        "        Q1 = np.percentile(series, 25)\n",
        "        Q3 = np.percentile(series, 75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        lower_bound = Q1 - k * IQR\n",
        "        upper_bound = Q3 + k * IQR\n",
        "\n",
        "        is_outlier = (series < lower_bound) | (series > upper_bound)\n",
        "        bounds_info = {\n",
        "            'Q1': Q1,\n",
        "            'Q3': Q3,\n",
        "            'IQR': IQR,\n",
        "            'Lower_Bound': lower_bound,\n",
        "            'Upper_Bound': upper_bound\n",
        "        }\n",
        "        return is_outlier.reindex(data.index, fill_value=False), bounds_info\n",
        "\n",
        "    elif isinstance(data, np.ndarray):\n",
        "        if data.ndim > 1:\n",
        "            print(\"Warning: IQR detection for NumPy array assumes 1D data. Processing each column if 2D.\")\n",
        "            outliers_array = np.zeros_like(data, dtype=bool)\n",
        "            bounds_info_array = {}\n",
        "            for col_idx in range(data.shape[1]):\n",
        "                col_data = data[:, col_idx]\n",
        "                valid_data = col_data[~np.isnan(col_data)]\n",
        "                if valid_data.size == 0:\n",
        "                    outliers_array[:, col_idx] = False\n",
        "                    bounds_info_array[f'col_{col_idx}'] = {'Q1': np.nan, 'Q3': np.nan, 'IQR': np.nan, 'Lower_Bound': np.nan, 'Upper_Bound': np.nan}\n",
        "                    continue\n",
        "\n",
        "                Q1 = np.percentile(valid_data, 25)\n",
        "                Q3 = np.percentile(valid_data, 75)\n",
        "                IQR = Q3 - Q1\n",
        "\n",
        "                lower_bound = Q1 - k * IQR\n",
        "                upper_bound = Q3 + k * IQR\n",
        "\n",
        "                is_outlier = (col_data < lower_bound) | (col_data > upper_bound)\n",
        "                outliers_array[:, col_idx] = is_outlier\n",
        "                bounds_info_array[f'col_{col_idx}'] = {\n",
        "                    'Q1': Q1, 'Q3': Q3, 'IQR': IQR,\n",
        "                    'Lower_Bound': lower_bound, 'Upper_Bound': upper_bound\n",
        "                }\n",
        "            return outliers_array, bounds_info_array\n",
        "        else:\n",
        "            valid_data = data[~np.isnan(data)]\n",
        "            if valid_data.size == 0:\n",
        "                return np.full_like(data, False, dtype=bool), {'Q1': np.nan, 'Q3': np.nan, 'IQR': np.nan, 'Lower_Bound': np.nan, 'Upper_Bound': np.nan}\n",
        "\n",
        "            Q1 = np.percentile(valid_data, 25)\n",
        "            Q3 = np.percentile(valid_data, 75)\n",
        "            IQR = Q3 - Q1\n",
        "\n",
        "            lower_bound = Q1 - k * IQR\n",
        "            upper_bound = Q3 + k * IQR\n",
        "\n",
        "            is_outlier = (data < lower_bound) | (data > upper_bound)\n",
        "            bounds_info = {\n",
        "                'Q1': Q1,\n",
        "                'Q3': Q3,\n",
        "                'IQR': IQR,\n",
        "                'Lower_Bound': lower_bound,\n",
        "                'Upper_Bound': upper_bound\n",
        "            }\n",
        "            return is_outlier, bounds_info\n",
        "    else:\n",
        "        raise TypeError(\"Input data must be a pandas Series, DataFrame, or NumPy array.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        df3 = pd.read_csv(\"data3.csv\")\n",
        "        print(\"Original data from data3.csv:\")\n",
        "        print(df3.head())\n",
        "\n",
        "        numerical_data = df3.select_dtypes(include=np.number)\n",
        "\n",
        "        if numerical_data.empty:\n",
        "            print(\"No numerical columns found in data3.csv for IQR outlier detection.\")\n",
        "        else:\n",
        "            iqr_k_value = 1.5\n",
        "            print(f\"\\nPerforming IQR outlier detection with k={iqr_k_value}...\")\n",
        "            is_outlier_iqr, bounds_iqr = iqr_outlier_detection(numerical_data, k=iqr_k_value)\n",
        "\n",
        "            print(\"\\nIs Outlier (IQR method):\")\n",
        "            print(is_outlier_iqr.head())\n",
        "\n",
        "            print(\"\\nCalculated IQR Bounds per column:\")\n",
        "            for col, info in bounds_iqr.items():\n",
        "                print(f\"  Column '{col}':\")\n",
        "                for key, value in info.items():\n",
        "                    print(f\"    {key}: {value:.2f}\")\n",
        "\n",
        "            outlier_rows_iqr = df3[is_outlier_iqr.any(axis=1)]\n",
        "            if not outlier_rows_iqr.empty:\n",
        "                print(\"\\nRows identified as outliers (IQR method):\")\n",
        "                print(outlier_rows_iqr)\n",
        "            else:\n",
        "                print(\"\\nNo outliers found using IQR method with the given k value.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: data3.csv not found. Please make sure the file is in the same directory.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpmnTqTzXK0T",
        "outputId": "3aa3d7eb-99ae-41dd-e03c-999e9b6bde6c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data from data3.csv:\n",
            "   Value\n",
            "0    1.0\n",
            "1    2.0\n",
            "2    3.0\n",
            "3    4.0\n",
            "4   10.0\n",
            "\n",
            "Performing IQR outlier detection with k=1.5...\n",
            "\n",
            "Is Outlier (IQR method):\n",
            "   Value\n",
            "0  False\n",
            "1  False\n",
            "2  False\n",
            "3  False\n",
            "4  False\n",
            "\n",
            "Calculated IQR Bounds per column:\n",
            "  Column 'Value':\n",
            "    Q1: -0.76\n",
            "    Q3: 4.25\n",
            "    IQR: 5.00\n",
            "    Lower_Bound: -8.26\n",
            "    Upper_Bound: 11.76\n",
            "\n",
            "Rows identified as outliers (IQR method):\n",
            "    Value\n",
            "6   -10.0\n",
            "14  -20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mini-Batch K-Means Algorithm"
      ],
      "metadata": {
        "id": "Mg2qOm0ZXNRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def euclidean_distance(point1, point2):\n",
        "    return np.sqrt(np.sum((point1 - point2)**2))\n",
        "\n",
        "def assign_to_clusters(data, centroids):\n",
        "    assignments = np.zeros(data.shape[0], dtype=int)\n",
        "    for i, point in enumerate(data):\n",
        "        distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
        "        assignments[i] = np.argmin(distances)\n",
        "    return assignments\n",
        "\n",
        "def mini_batch_kmeans(data, k, batch_size, max_iterations=100, random_state=None):\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        data_np = data.values\n",
        "    else:\n",
        "        data_np = data\n",
        "\n",
        "    n_samples, n_features = data_np.shape\n",
        "\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    initial_centroid_indices = np.random.choice(n_samples, k, replace=False)\n",
        "    centroids = data_np[initial_centroid_indices].astype(float)\n",
        "\n",
        "    centroid_counts = np.ones(k)\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        batch_indices = np.random.choice(n_samples, batch_size, replace=False)\n",
        "        mini_batch = data_np[batch_indices]\n",
        "\n",
        "        batch_assignments = assign_to_clusters(mini_batch, centroids)\n",
        "\n",
        "        for i, point in enumerate(mini_batch):\n",
        "            assigned_cluster_id = batch_assignments[i]\n",
        "            centroids[assigned_cluster_id] = (centroids[assigned_cluster_id] * centroid_counts[assigned_cluster_id] + point) / (centroid_counts[assigned_cluster_id] + 1)\n",
        "            centroid_counts[assigned_cluster_id] += 1\n",
        "\n",
        "    final_assignments = assign_to_clusters(data_np, centroids)\n",
        "\n",
        "    return final_assignments, centroids\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        df4 = pd.read_csv(\"data4.csv\")\n",
        "        data_for_mini_batch_kmeans = df4.select_dtypes(include=np.number)\n",
        "\n",
        "        if data_for_mini_batch_kmeans.empty:\n",
        "            print(\"No numerical columns found in data4.csv for Mini-Batch K-Means.\")\n",
        "        else:\n",
        "            data_for_mini_batch_kmeans = data_for_mini_batch_kmeans.dropna()\n",
        "\n",
        "            if data_for_mini_batch_kmeans.empty:\n",
        "                print(\"No valid numerical data after dropping NaNs in data4.csv.\")\n",
        "            else:\n",
        "                k_value = 3\n",
        "                batch_size_value = 50\n",
        "\n",
        "                if batch_size_value > len(data_for_mini_batch_kmeans):\n",
        "                    batch_size_value = len(data_for_mini_batch_kmeans)\n",
        "                    print(f\"Adjusted batch_size to {batch_size_value} as it was larger than the dataset size.\")\n",
        "\n",
        "                print(f\"Running Mini-Batch K-Means with k={k_value}, batch_size={batch_size_value} on data4.csv...\")\n",
        "                cluster_assignments, final_centroids = mini_batch_kmeans(\n",
        "                    data_for_mini_batch_kmeans,\n",
        "                    k=k_value,\n",
        "                    batch_size=batch_size_value,\n",
        "                    random_state=42\n",
        "                )\n",
        "\n",
        "                print(\"\\nFinal Cluster Assignments (Mini-Batch K-Means):\")\n",
        "                print(cluster_assignments)\n",
        "\n",
        "                print(\"\\nFinal Centroids (Mini-Batch K-Means):\")\n",
        "                print(final_centroids)\n",
        "\n",
        "                df4['MiniBatchKMeans_Cluster'] = pd.Series(cluster_assignments, index=data_for_mini_batch_kmeans.index)\n",
        "                print(\"\\nDataFrame with Mini-Batch K-Means Clusters:\")\n",
        "                print(df4.head())\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: data4.csv not found. Please make sure the file is in the same directory.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZC9BSeiXPYA",
        "outputId": "90635653-6a10-47ad-8db0-53e5d4f36729"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Mini-Batch K-Means with k=3, batch_size=50 on data4.csv...\n",
            "\n",
            "Final Cluster Assignments (Mini-Batch K-Means):\n",
            "[2 2 1 1 0 0 2 0 2 0 2 2 2 2 2 1 2 0 2 1 2 2 1 2 1 2 2 0 2 2 2 2 0 1 2 2 1\n",
            " 2 1 2 2 2 0 2 0 2 2 1 2 2 1 2 1 2 2 2 1 2 2 2 1 2 0 1 1 1 2 2 1 2 2 1 2 2\n",
            " 2 2 2 2 2 2 2 2 2 1 0 1 1 2 2 0 2 2 2 2 2 1 2 2 2 2 1 0 2 1 2 2 0 2 2 1 2\n",
            " 2 2 1 2 1 0 2 2 2 1 2 2 1 0 2 2 2 2 2 2 0 2 0 1 2 2 2 2 2 2 2 0 2 2 2 2 1\n",
            " 0 2 2 1 2 0 2 0 1 2 2 2 2 2 2 0 2 2 2 2 1 2 1 2 2 1 2 2 2 2 2 2 2 2 1 0 0\n",
            " 2 2 2 2 2 2 2 0 0 2 0 2 2 2 2]\n",
            "\n",
            "Final Centroids (Mini-Batch K-Means):\n",
            "[[-6.92499219 -6.06989889]\n",
            " [-6.61263356 -7.44574048]\n",
            " [ 0.97702332  5.5431696 ]]\n",
            "\n",
            "DataFrame with Mini-Batch K-Means Clusters:\n",
            "   Feature_1  Feature_2  MiniBatchKMeans_Cluster\n",
            "0   6.505653   2.447003                        2\n",
            "1  -5.128943   9.836189                        2\n",
            "2  -6.891874  -7.777364                        1\n",
            "3  -8.327712  -8.287573                        1\n",
            "4  -7.468992  -6.030507                        0\n"
          ]
        }
      ]
    }
  ]
}